#!/usr/bin/env python3
"""
MBPPè¯„ä¼°è„šæœ¬ - Baseæ¨¡å‹ç‰ˆæœ¬

é’ˆå¯¹Qwen3 0.6B Baseç­‰é¢„è®­ç»ƒæ¨¡å‹çš„è¯„ä¼°ã€‚
æä¾›äº†å®Œæ•´çš„MBPPæ•°æ®é›†è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒ0-shotä»£ç ç”Ÿæˆpass@kè¯„ä¼°ã€‚

Example:
    åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼ˆpass@1ï¼‰ï¼š
    ```bash
    python run_mbpp_original_base.py --model Qwen/Qwen3-0.6B-Base --k 1
    ```

    pass@10è¯„ä¼°ï¼š
    ```bash
    python run_mbpp_original_base.py --model Qwen/Qwen3-0.6B-Base --k 10
    ```

    é™åˆ¶æ ·æœ¬æ•°é‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼š
    ```bash
    python run_mbpp_original_base.py --model Qwen/Qwen3-0.6B-Base --max-samples 10 --k 5
    ```
"""

from __future__ import annotations

import argparse
import json
import logging
import signal
import time
from dataclasses import dataclass, field
from functools import lru_cache
from pathlib import Path
from typing import Any, NamedTuple

import datasets
import torch
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


# ================================ é…ç½®å’Œå¸¸é‡ ================================


@dataclass(frozen=True)
class GenerationConfig:
    """ä»£ç ç”Ÿæˆé…ç½®ç±»ã€‚

    åŒ…å«æ‰€æœ‰ç”¨äºæ§åˆ¶æ¨¡å‹ç”Ÿæˆè¡Œä¸ºçš„å‚æ•°ã€‚
    """

    temperature: float = 0.2
    do_sample: bool = True
    top_p: float = 0.95
    top_k: int = 50
    repetition_penalty: float = 1.05
    no_repeat_ngram_size: int = 3
    early_stopping: bool = False


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®ç±»ã€‚

    åŒ…å«è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å¯é…ç½®å‚æ•°ã€‚
    """

    model_path: str
    k: int = 1  # pass@kä¸­çš„kå€¼
    output_dir: str = "mbpp_results_base"
    max_samples: int | None = None
    max_new_tokens: int = 512
    timeout_seconds: int = 5
    generation_config: GenerationConfig = field(default_factory=GenerationConfig)


class EvaluationResult(NamedTuple):
    """å•ä¸ªè¯„ä¼°æ ·æœ¬çš„ç»“æœã€‚"""

    task_id: int
    prompt: str
    generated_codes: list[str]  # å­˜å‚¨kæ¬¡ç”Ÿæˆçš„ä»£ç 
    test_cases: list[str]
    test_results: list[bool]  # å­˜å‚¨kæ¬¡æµ‹è¯•çš„ç»“æœ
    test_passed: bool  # æ˜¯å¦è‡³å°‘æœ‰ä¸€æ¬¡é€šè¿‡ï¼ˆpass@kï¼‰
    code_lengths: list[int]  # å­˜å‚¨kæ¬¡ç”Ÿæˆä»£ç çš„é•¿åº¦
    generation_times: list[float]  # å­˜å‚¨kæ¬¡ç”Ÿæˆçš„æ—¶é—´


class EvaluationSummary(NamedTuple):
    """è¯„ä¼°æ€»ç»“ç»“æœã€‚"""

    model: str
    model_type: str
    evaluation_method: str
    dataset: str
    k: int  # pass@kä¸­çš„kå€¼
    total_samples: int
    passed_samples: int
    pass_at_k: float
    evaluation_time: float
    average_time_per_sample: float


# ================================ è‡ªå®šä¹‰å¼‚å¸¸ ================================


class MBPPEvaluationError(Exception):
    """MBPPè¯„ä¼°è¿‡ç¨‹ä¸­çš„åŸºç¡€å¼‚å¸¸ç±»ã€‚"""

    pass


class ModelLoadError(MBPPEvaluationError):
    """æ¨¡å‹åŠ è½½å¤±è´¥å¼‚å¸¸ã€‚"""

    pass


class DatasetLoadError(MBPPEvaluationError):
    """æ•°æ®é›†åŠ è½½å¤±è´¥å¼‚å¸¸ã€‚"""

    pass


class CodeGenerationError(MBPPEvaluationError):
    """ä»£ç ç”Ÿæˆå¤±è´¥å¼‚å¸¸ã€‚"""

    pass


class CodeExecutionTimeoutError(MBPPEvaluationError):
    """ä»£ç æ‰§è¡Œè¶…æ—¶å¼‚å¸¸ã€‚"""

    pass


# ================================ æ ¸å¿ƒåŠŸèƒ½ç±» ================================


class ModelInterface:
    """æ¨¡å‹æ¥å£ç±»ï¼Œå°è£…æ¨¡å‹åŠ è½½å’Œä»£ç ç”ŸæˆåŠŸèƒ½ã€‚"""

    def __init__(self, model_path: str) -> None:
        """åˆå§‹åŒ–æ¨¡å‹æ¥å£ã€‚

        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°

        Raises:
            ModelLoadError: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        """
        self.model_path = model_path
        self._model: AutoModelForCausalLM | None = None
        self._tokenizer: AutoTokenizer | None = None
        self._load_model()

    def _load_model(self) -> None:
        """åŠ è½½Baseæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

        Raises:
            ModelLoadError: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        """
        logger.info(f"æ­£åœ¨åŠ è½½Baseæ¨¡å‹: {self.model_path}")

        try:
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self._model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=(
                    torch.float16 if torch.cuda.is_available() else torch.float32
                ),
                device_map="auto" if torch.cuda.is_available() else None,
            )
            logger.info("âœ… Baseæ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            error_msg = f"Baseæ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            raise ModelLoadError(error_msg) from e

    @property
    def model(self) -> AutoModelForCausalLM:
        """è·å–æ¨¡å‹å®ä¾‹ã€‚"""
        if self._model is None:
            raise ModelLoadError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
        return self._model

    @property
    def tokenizer(self) -> AutoTokenizer:
        """è·å–åˆ†è¯å™¨å®ä¾‹ã€‚"""
        if self._tokenizer is None:
            raise ModelLoadError("åˆ†è¯å™¨æœªæ­£ç¡®åŠ è½½")
        return self._tokenizer

    def generate_code(
        self, prompt: str, config: GenerationConfig, max_new_tokens: int = 256
    ) -> str:
        """ç”Ÿæˆä»£ç ã€‚

        Args:
            prompt: è¾“å…¥æç¤º
            config: ç”Ÿæˆé…ç½®
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°é‡

        Returns:
            ç”Ÿæˆçš„ä»£ç å­—ç¬¦ä¸²

        Raises:
            CodeGenerationError: å½“ä»£ç ç”Ÿæˆå¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            # ç¼–ç è¾“å…¥å¹¶è·å–attention_mask
            inputs = self.tokenizer(
                prompt, return_tensors="pt", padding=True, truncation=True
            )

            if hasattr(self.model, "device"):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            # ç”Ÿæˆä»£ç 
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=max_new_tokens,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_return_sequences=1,
                    **config.__dict__,
                )

            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return self._clean_generated_code(generated)

        except Exception as e:
            error_msg = f"ä»£ç ç”Ÿæˆå¤±è´¥: {e}"
            logger.warning(error_msg)
            raise CodeGenerationError(error_msg) from e

    @staticmethod
    def _clean_generated_code(generated_text: str) -> str:
        """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–å¹¶æ¸…ç†ä»£ç ã€‚

        è¯¥æ–¹æ³•ä¼šæ‰¾åˆ°å‡½æ•°å®šä¹‰ï¼Œæ”¶é›†å‡½æ•°ä½“ï¼Œå¹¶æ‰¾åˆ°æœ€åä¸€ä¸ªreturnè¯­å¥ï¼Œ
        åˆ å»returnä¹‹åçš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯ç¤ºä¾‹ä»£ç ï¼‰ã€‚

        Args:
            generated_text: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬

        Returns:
            æ¸…ç†åçš„ä»£ç å­—ç¬¦ä¸²
        """

        lines = generated_text.split("\n")

        # æ‰¾åˆ°å‡½æ•°å®šä¹‰çš„èµ·å§‹ä½ç½®
        def_start_index = next(
            (i for i, line in enumerate(lines) if line.strip().startswith("def ")), -1
        )

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å‡½æ•°å®šä¹‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if def_start_index == -1:
            return ""

        # å®šä¹‰éœ€è¦æ’é™¤çš„è¡Œå¼€å¤´æ¨¡å¼
        exclude_patterns = ("print", "#")

        # ä»å‡½æ•°å®šä¹‰å¼€å§‹ï¼Œæå–å’Œè¿‡æ»¤ä»£ç è¡Œï¼ˆä¸€æ¬¡éå†å®Œæˆæ‰€æœ‰æ“ä½œï¼‰
        code_lines = []
        last_return_index = -1

        for line in lines[def_start_index:]:
            stripped_line = line.strip()

            # è·³è¿‡ç©ºè¡Œå’Œéœ€è¦æ’é™¤çš„è¡Œ
            if not stripped_line or stripped_line.startswith(exclude_patterns):
                continue

            # æ·»åŠ æœ‰æ•ˆçš„ä»£ç è¡Œ
            code_lines.append(line)

            # è®°å½•returnè¯­å¥çš„ä½ç½®ï¼ˆé¿å…äºŒæ¬¡éå†ï¼‰
            if stripped_line.startswith("return "):
                last_return_index = len(code_lines) - 1

        # æˆªå–åˆ°æœ€åä¸€ä¸ªreturnè¯­å¥ä¸ºæ­¢ï¼Œå¦‚æœæ²¡æœ‰returnåˆ™è¿”å›æ‰€æœ‰ä»£ç è¡Œ
        return "\n".join(
            code_lines[: last_return_index + 1]
            if last_return_index >= 0
            else code_lines
        )


class CodeExecutor:
    """ä»£ç æ‰§è¡Œå™¨ç±»ï¼Œè´Ÿè´£å®‰å…¨æ‰§è¡Œå’Œæµ‹è¯•ç”Ÿæˆçš„ä»£ç ã€‚"""

    def __init__(self, timeout_seconds: int = 5) -> None:
        """åˆå§‹åŒ–ä»£ç æ‰§è¡Œå™¨ã€‚

        Args:
            timeout_seconds: ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.timeout_seconds = timeout_seconds

    def test_code_execution(self, code: str, test_cases: list[str]) -> bool:
        """æ‰§è¡Œä»£ç æµ‹è¯•ï¼Œè®¾ç½®è¶…æ—¶é¿å…æ­»å¾ªç¯ã€‚

        Args:
            code: è¦æµ‹è¯•çš„ä»£ç 
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨

        Returns:
            Trueå¦‚æœæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼Œå¦åˆ™False

        Raises:
            CodeExecutionTimeoutError: å½“ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶æŠ›å‡º
        """

        def timeout_handler(signum: int, frame: Any) -> None:
            raise CodeExecutionTimeoutError(f"ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆ{self.timeout_seconds}ç§’ï¼‰")

        try:
            namespace: dict[str, Any] = {}

            # è®¾ç½®è¶…æ—¶å¤„ç†
            old_handler = signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(self.timeout_seconds)

            try:
                # æ‰§è¡Œä»£ç 
                exec(code, namespace)

                # æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
                for i, test_case in enumerate(test_cases):
                    try:
                        exec(test_case, namespace)
                        logger.debug(f"æµ‹è¯•ç”¨ä¾‹ {i+1} é€šè¿‡")
                    except Exception as e:
                        logger.debug(f"æµ‹è¯•ç”¨ä¾‹ {i+1} å¤±è´¥: {e}")
                        logger.debug(f"æµ‹è¯•ç”¨ä¾‹å†…å®¹: {test_case}")
                        return False

                return True

            finally:
                # æ¢å¤åŸæ¥çš„ä¿¡å·å¤„ç†å™¨å¹¶å–æ¶ˆè¶…æ—¶
                signal.alarm(0)
                signal.signal(signal.SIGALRM, old_handler)

        except CodeExecutionTimeoutError:
            logger.debug(f"ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆ{self.timeout_seconds}ç§’ï¼‰ï¼Œå¯èƒ½å­˜åœ¨æ­»å¾ªç¯")
            return False
        except Exception as e:
            logger.debug(f"æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
            return False


class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨ç±»ã€‚"""

    @staticmethod
    @lru_cache(maxsize=1)
    def load_mbpp_dataset(max_samples: int | None = None) -> datasets.Dataset:
        """åŠ è½½MBPPæ•°æ®é›†ï¼ˆå¸¦ç¼“å­˜ï¼‰ã€‚

        Args:
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨

        Returns:
            MBPPæ•°æ®é›†

        Raises:
            DatasetLoadError: å½“æ•°æ®é›†åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            dataset = load_dataset("mbpp", split="test")
            logger.info(f"âœ… MBPPæ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

            if max_samples is not None:
                dataset = dataset.select(range(min(max_samples, len(dataset))))
                logger.info(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ä¸º: {max_samples}")

            return dataset

        except Exception as e:
            error_msg = f"MBPPæ•°æ®é›†åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            raise DatasetLoadError(error_msg) from e


class MBPPEvaluator:
    """MBPPè¯„ä¼°å™¨ä¸»ç±»ã€‚"""

    def __init__(self, config: EvaluationConfig) -> None:
        """åˆå§‹åŒ–è¯„ä¼°å™¨ã€‚

        Args:
            config: è¯„ä¼°é…ç½®
        """
        self.config = config
        self.model_interface = ModelInterface(config.model_path)
        self.code_executor = CodeExecutor(config.timeout_seconds)
        self.dataset_loader = DatasetLoader()

    def evaluate(self) -> EvaluationSummary:
        """æ‰§è¡Œå®Œæ•´çš„MBPPè¯„ä¼°ã€‚

        Returns:
            è¯„ä¼°æ€»ç»“ç»“æœ

        Raises:
            MBPPEvaluationError: å½“è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡º
        """
        try:
            # åŠ è½½æ•°æ®é›†
            dataset = self.dataset_loader.load_mbpp_dataset(self.config.max_samples)

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(self.config.output_dir)
            output_path.mkdir(exist_ok=True)

            # å¼€å§‹è¯„ä¼°
            logger.info(f"å¼€å§‹MBPP Baseæ¨¡å‹0-shotè¯„ä¼°ï¼ˆpass@{self.config.k}ï¼‰...")
            results: list[EvaluationResult] = []
            total_correct = 0
            start_time = time.time()

            for i, sample in enumerate(tqdm(dataset, desc="MBPPè¯„ä¼°")):
                result = self._evaluate_single_sample(sample)
                results.append(result)

                if result.test_passed:
                    total_correct += 1

                # å®šæœŸæ‰“å°è¿›åº¦
                if (i + 1) % 10 == 0:
                    current_pass_rate = total_correct / (i + 1)
                    logger.info(
                        f"è¿›åº¦: {i+1}/{len(dataset)}, å½“å‰pass@{self.config.k}: {current_pass_rate:.3f}"
                    )

            # è®¡ç®—å’Œä¿å­˜ç»“æœ
            return self._save_evaluation_results(
                results, start_time, total_correct, output_path
            )

        except Exception as e:
            if isinstance(e, MBPPEvaluationError):
                raise
            error_msg = f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}"
            logger.error(error_msg)
            raise MBPPEvaluationError(error_msg) from e

    def _evaluate_single_sample(self, sample: dict[str, Any]) -> EvaluationResult:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬ã€‚

        Args:
            sample: æ•°æ®é›†ä¸­çš„å•ä¸ªæ ·æœ¬

        Returns:
            å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ
        """
        # æ„é€ æç¤º
        prompt = self._build_prompt(sample)

        # è¿›è¡Œkæ¬¡ä»£ç ç”Ÿæˆå’Œæµ‹è¯•
        generated_codes = []
        test_results = []
        code_lengths = []
        generation_times = []

        for i in range(self.config.k):
            # ç”Ÿæˆä»£ç 
            generation_start = time.time()
            try:
                generated_code = self.model_interface.generate_code(
                    prompt, self.config.generation_config, self.config.max_new_tokens
                )
            except CodeGenerationError as e:
                logger.warning(f"ä»»åŠ¡{sample['task_id']}ç¬¬{i+1}æ¬¡ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
                generated_code = "# ä»£ç ç”Ÿæˆå¤±è´¥\npass"

            generation_time = time.time() - generation_start

            # æµ‹è¯•ä»£ç 
            test_passed = self.code_executor.test_code_execution(
                generated_code, sample["test_list"]
            )

            # å­˜å‚¨ç»“æœ
            generated_codes.append(generated_code)
            test_results.append(test_passed)
            code_lengths.append(len(generated_code))
            generation_times.append(generation_time)

        # è®¡ç®—pass@k: å¦‚æœkæ¬¡ç”Ÿæˆä¸­è‡³å°‘æœ‰ä¸€æ¬¡é€šè¿‡æµ‹è¯•ï¼Œåˆ™è¯¥æ ·æœ¬é€šè¿‡
        sample_passed = any(test_results)

        return EvaluationResult(
            task_id=sample["task_id"],
            prompt=prompt,
            generated_codes=generated_codes,
            test_cases=sample["test_list"],
            test_results=test_results,
            test_passed=sample_passed,
            code_lengths=code_lengths,
            generation_times=generation_times,
        )

    @staticmethod
    def _build_prompt(sample: dict[str, Any]) -> str:
        """æ„å»ºè¯„ä¼°æç¤ºã€‚

        Args:
            sample: æ•°æ®é›†æ ·æœ¬

        Returns:
            æ„å»ºå¥½çš„æç¤ºå­—ç¬¦ä¸²
        """
        code = sample["code"]

        # æå–å‡½æ•°ç­¾å
        for line in code.split("\n"):
            if line.strip().startswith("def"):
                code_begin = line
                break
        else:
            code_begin = ""

        return sample["text"] + "\n" + code_begin

    def _save_evaluation_results(
        self,
        results: list[EvaluationResult],
        start_time: float,
        total_correct: int,
        output_path: Path,
    ) -> EvaluationSummary:
        """ä¿å­˜è¯„ä¼°ç»“æœå¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚

        Args:
            results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            start_time: è¯„ä¼°å¼€å§‹æ—¶é—´
            total_correct: é€šè¿‡çš„æ ·æœ¬æ•°é‡
            output_path: è¾“å‡ºè·¯å¾„

        Returns:
            è¯„ä¼°æ€»ç»“
        """
        total_time = time.time() - start_time
        pass_rate = total_correct / len(results) if results else 0.0

        evaluation_summary = EvaluationSummary(
            model=self.config.model_path,
            model_type="base",
            evaluation_method="0-shot",
            dataset="mbpp_original",
            k=self.config.k,
            total_samples=len(results),
            passed_samples=total_correct,
            pass_at_k=pass_rate,
            evaluation_time=total_time,
            average_time_per_sample=total_time / len(results) if results else 0,
        )

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ç”¨äºä¿å­˜
        summary_dict = evaluation_summary._asdict()
        results_dict = [result._asdict() for result in results]

        # ä¿å­˜æ–‡ä»¶
        results_file = output_path / "mbpp_base_evaluation_results.json"
        detailed_file = output_path / "mbpp_base_detailed_results.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary_dict, f, ensure_ascii=False, indent=2)

        with open(detailed_file, "w", encoding="utf-8") as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_evaluation_summary(evaluation_summary, results_file, detailed_file)

        return evaluation_summary

    @staticmethod
    def _print_evaluation_summary(
        summary: EvaluationSummary,
        results_file: Path,
        detailed_file: Path,
    ) -> None:
        """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦ã€‚

        Args:
            summary: è¯„ä¼°æ€»ç»“
            results_file: ç»“æœæ–‡ä»¶è·¯å¾„
            detailed_file: è¯¦ç»†ç»“æœæ–‡ä»¶è·¯å¾„
        """
        logger.info("=" * 50)
        logger.info(f"MBPP Baseæ¨¡å‹0-shotè¯„ä¼°å®Œæˆï¼ˆpass@{summary.k}ï¼‰ï¼")
        logger.info("=" * 50)
        logger.info(f"æ¨¡å‹: {summary.model}")
        logger.info(f"æ¨¡å‹ç±»å‹: Base (é¢„è®­ç»ƒæ¨¡å‹)")
        logger.info(f"è¯„ä¼°æ–¹æ³•: 0-shot pass@{summary.k}")
        logger.info(f"æ€»æ ·æœ¬æ•°: {summary.total_samples}")
        logger.info(f"é€šè¿‡æ ·æœ¬æ•°: {summary.passed_samples}")
        logger.info(f"Pass@{summary.k}: {summary.pass_at_k:.3f}")
        logger.info(f"è¯„ä¼°ç”¨æ—¶: {summary.evaluation_time:.2f}ç§’")
        logger.info(f"å¹³å‡æ¯æ ·æœ¬: {summary.average_time_per_sample:.2f}ç§’")
        logger.info(f"ç»“æœæ–‡ä»¶: {results_file}")
        logger.info(f"è¯¦ç»†ç»“æœ: {detailed_file}")
        logger.info("=" * 50)


# ================================ å‘½ä»¤è¡Œæ¥å£ ================================


def create_arg_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚

    Returns:
        é…ç½®å¥½çš„å‚æ•°è§£æå™¨
    """
    parser = argparse.ArgumentParser(
        description="MBPP Baseæ¨¡å‹0-shot pass@kè¯„ä¼°è„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--model",
        default="Qwen/Qwen3-0.6B-Base",
        help="Baseæ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å",
    )

    parser.add_argument(
        "--output-dir",
        default="mbpp_results_base",
        help="è¾“å‡ºç›®å½•",
    )

    parser.add_argument(
        "--max-samples",
        type=int,
        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼Œé»˜è®¤è¯„ä¼°å…¨éƒ¨ï¼‰",
    )

    parser.add_argument(
        "--max-length",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆæ–°tokenæ•°é‡",
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=10,
        help="ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰",
    )

    parser.add_argument(
        "--k",
        type=int,
        default=1,
        help="pass@kè¯„ä¼°ä¸­çš„kå€¼ï¼ˆé»˜è®¤ä¸º1ï¼‰",
    )

    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ«",
    )

    return parser


def setup_logging(log_level: str) -> None:
    """è®¾ç½®æ—¥å¿—é…ç½®ã€‚

    Args:
        log_level: æ—¥å¿—çº§åˆ«
    """
    logging.basicConfig(
        level=getattr(logging, log_level),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    parser = create_arg_parser()
    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)

    # åˆ›å»ºè¯„ä¼°é…ç½®
    config = EvaluationConfig(
        model_path=args.model,
        k=args.k,
        output_dir=args.output_dir,
        max_samples=args.max_samples,
        max_new_tokens=args.max_length,
        timeout_seconds=args.timeout,
    )

    try:
        # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
        evaluator = MBPPEvaluator(config)
        results = evaluator.evaluate()

        # æ‰“å°æˆåŠŸä¿¡æ¯
        print(f"\nğŸ‰ MBPP Baseæ¨¡å‹0-shotè¯„ä¼°æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“Š Pass@{results.k}: {results.pass_at_k:.3f}")
        print(f"ğŸ¤– æ¨¡å‹ç±»å‹: Base (é¢„è®­ç»ƒæ¨¡å‹)")
        print(f"ğŸ¯ è¯„ä¼°æ–¹æ³•: 0-shot pass@{results.k}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}")

    except MBPPEvaluationError as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        print(f"\nâŒ MBPP Baseæ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
        exit(1)
    except Exception as e:
        logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {e}")
        print(f"\nâŒ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        exit(1)


# ================================ æ—¥å¿—é…ç½® ================================

logger = logging.getLogger(__name__)


# ================================ ä¸»ç¨‹åºå…¥å£ ================================

if __name__ == "__main__":
    main()
