#!/usr/bin/env python3
"""
MBPPåŸç‰ˆè¯„ä¼°è„šæœ¬ - Instructæ¨¡å‹ç‰ˆæœ¬
ä¸“é—¨é’ˆå¯¹Qwen3åè®­ç»ƒæ¨¡å‹çš„è¯„ä¼°ã€‚
è¯¥æ¨¡å—æä¾›äº†å®Œæ•´çš„MBPPæ•°æ®é›†è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒ0-shotä»£ç ç”Ÿæˆpass@kè¯„ä¼°ã€‚

Example:
    åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼ˆpass@1ï¼‰ï¼š
    ```bash
    python run_mbpp_original_instruct_non_thinking.py --model Qwen/Qwen3-0.6B --k 1
    ```

    pass@10è¯„ä¼°ï¼š
    ```bash
    python run_mbpp_original_instruct_non_thinking.py --model Qwen/Qwen3-0.6B --k 10
    ```

    é™åˆ¶æ ·æœ¬æ•°é‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•ï¼š
    ```bash
    python run_mbpp_original_instruct_non_thinking.py --model Qwen/Qwen3-0.6B --max-samples 10 --k 5
    ```
"""

from __future__ import annotations

import re
import argparse
import json
import logging
import signal
import time
from dataclasses import dataclass, field
from functools import lru_cache
from pathlib import Path
from typing import Any, NamedTuple

import datasets
import torch
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


# ================================ é…ç½®å’Œå¸¸é‡ ================================


@dataclass(frozen=True)
class GenerationConfig:
    """ä»£ç ç”Ÿæˆé…ç½®ç±»ã€‚

    åŒ…å«ç”¨äºæ§åˆ¶æ¨¡å‹ç”Ÿæˆè¡Œä¸ºçš„å‚æ•°ã€‚
    """

    # Best practices from huggingface
    temperature: float = 0.7
    top_p: float = 0.8
    top_k: int = 20
    min_p: float = 0.0


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®ç±»ã€‚
    åŒ…å«è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å¯é…ç½®å‚æ•°ã€‚
    """

    model_path: str
    k: int = 1  # pass@kä¸­çš„kå€¼
    output_dir: str = "mbpp_results_instruct_non_thinking"
    max_samples: int | None = None
    max_new_tokens: int = 512
    timeout_seconds: int = 5
    generation_config: GenerationConfig = field(default_factory=GenerationConfig)


class EvaluationResult(NamedTuple):
    """å•ä¸ªè¯„ä¼°æ ·æœ¬çš„ç»“æœã€‚"""

    task_id: int
    prompt: str
    generated_codes: list[str]  # å­˜å‚¨kæ¬¡ç”Ÿæˆçš„ä»£ç 
    test_cases: list[str]
    test_results: list[bool]  # å­˜å‚¨kæ¬¡æµ‹è¯•çš„ç»“æœ
    test_passed: bool  # æ˜¯å¦è‡³å°‘æœ‰ä¸€æ¬¡é€šè¿‡ï¼ˆpass@kï¼‰
    code_lengths: list[int]  # å­˜å‚¨kæ¬¡ç”Ÿæˆä»£ç çš„é•¿åº¦
    generation_times: list[float]  # å­˜å‚¨kæ¬¡ç”Ÿæˆçš„æ—¶é—´


class EvaluationSummary(NamedTuple):
    """è¯„ä¼°æ€»ç»“ç»“æœã€‚"""

    model: str
    model_type: str
    evaluation_method: str
    dataset: str
    k: int  # pass@kä¸­çš„kå€¼
    total_samples: int
    passed_samples: int
    pass_at_k: float
    evaluation_time: float
    average_time_per_sample: float


# ================================ è‡ªå®šä¹‰å¼‚å¸¸ ================================


class MBPPEvaluationError(Exception):
    """MBPPè¯„ä¼°è¿‡ç¨‹ä¸­çš„åŸºç¡€å¼‚å¸¸ç±»ã€‚"""

    pass


class ModelLoadError(MBPPEvaluationError):
    """æ¨¡å‹åŠ è½½å¤±è´¥å¼‚å¸¸ã€‚"""

    pass


class DatasetLoadError(MBPPEvaluationError):
    """æ•°æ®é›†åŠ è½½å¤±è´¥å¼‚å¸¸ã€‚"""

    pass


class CodeGenerationError(MBPPEvaluationError):
    """ä»£ç ç”Ÿæˆå¤±è´¥å¼‚å¸¸ã€‚"""

    pass


class CodeExecutionTimeoutError(MBPPEvaluationError):
    """ä»£ç æ‰§è¡Œè¶…æ—¶å¼‚å¸¸ã€‚"""

    pass


# ================================ æ ¸å¿ƒåŠŸèƒ½ç±» ================================


class ModelInterface:
    """æ¨¡å‹æ¥å£ç±»ï¼Œå°è£…æ¨¡å‹åŠ è½½å’Œä»£ç ç”ŸæˆåŠŸèƒ½ã€‚"""

    def __init__(self, model_path: str) -> None:
        """åˆå§‹åŒ–æ¨¡å‹æ¥å£ã€‚

        Args:
            model_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°

        Raises:
            ModelLoadError: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        """
        self.model_path = model_path
        self._model: AutoModelForCausalLM | None = None
        self._tokenizer: AutoTokenizer | None = None
        self._load_model()

    def _load_model(self) -> None:
        """åŠ è½½Instructæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

        Raises:
            ModelLoadError: å½“æ¨¡å‹åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        """
        logger.info(f"æ­£åœ¨åŠ è½½Instructæ¨¡å‹: {self.model_path}")

        try:
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self._model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=(
                    torch.float16 if torch.cuda.is_available() else torch.float32
                ),
                device_map="auto" if torch.cuda.is_available() else None,
            )

            logger.info("âœ… Instructæ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            error_msg = f"Instructæ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            raise ModelLoadError(error_msg) from e

    @property
    def model(self) -> AutoModelForCausalLM:
        """è·å–æ¨¡å‹å®ä¾‹ã€‚"""
        if self._model is None:
            raise ModelLoadError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
        return self._model

    @property
    def tokenizer(self) -> AutoTokenizer:
        """è·å–åˆ†è¯å™¨å®ä¾‹ã€‚"""
        if self._tokenizer is None:
            raise ModelLoadError("åˆ†è¯å™¨æœªæ­£ç¡®åŠ è½½")
        return self._tokenizer

    def generate_code(
        self,
        messages: list[dict[str, str]],
        config: GenerationConfig,
        max_new_tokens: int = 512,
    ) -> str:
        """ç”Ÿæˆä»£ç ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äº instruct æ¨¡å‹
            config: ç”Ÿæˆé…ç½®
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°é‡

        Returns:
            ç”Ÿæˆçš„ä»£ç å­—ç¬¦ä¸²

        Raises:
            CodeGenerationError: å½“ä»£ç ç”Ÿæˆå¤±è´¥æ—¶æŠ›å‡º
        """

        # å¯¹äº instruct æ¨¡å‹ï¼Œä½¿ç”¨ chat template
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False,  # Switches between thinking and non-thinking modes.
        )

        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        # ç”Ÿæˆä»£ç 
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                top_k=config.top_k,
                min_p=config.min_p,
            )

        # è§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()
        generated = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip(
            "\n"
        )

        return generated

    @staticmethod
    def _clean_generated_code(generated_text: str) -> str:
        """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–å¹¶æ¸…ç†ä»£ç ï¼ˆInstructæ¨¡å‹ï¼‰ã€‚

        Args:
            generated_text: æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬

        Returns:
            æ¸…ç†åçš„ä»£ç å­—ç¬¦ä¸²
        """

        code = generated_text.strip()

        # æå–markdownä»£ç å—
        markdown_patterns = [r"```python\s*\n(.*?)\n```", r"```\s*\n(.*?)\n```"]
        for pattern in markdown_patterns:
            matches = re.findall(pattern, code, re.DOTALL)
            if matches:
                code = max(matches, key=len).strip()
                break

        # æ¸…ç†ç©ºè¡Œ/æ³¨é‡Š/print
        lines = code.split("\n")
        cleaned_lines: list[str] = []
        for line in lines:
            stripped_line = line.strip()
            if not any(stripped_line.startswith(prefix) for prefix in ["print", "#"]):
                cleaned_lines.append(line)
        return "\n".join(cleaned_lines)


class CodeExecutor:
    """ä»£ç æ‰§è¡Œå™¨ç±»ï¼Œè´Ÿè´£å®‰å…¨æ‰§è¡Œå’Œæµ‹è¯•ç”Ÿæˆçš„ä»£ç ã€‚"""

    def __init__(self, timeout_seconds: int = 5) -> None:
        """åˆå§‹åŒ–ä»£ç æ‰§è¡Œå™¨ã€‚

        Args:
            timeout_seconds: ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.timeout_seconds = timeout_seconds

    def test_code_execution(self, code: str, test_cases: list[str]) -> bool:
        """æ‰§è¡Œä»£ç æµ‹è¯•ï¼Œè®¾ç½®è¶…æ—¶é¿å…æ­»å¾ªç¯ã€‚

        Args:
            code: è¦æµ‹è¯•çš„ä»£ç 
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨

        Returns:
            Trueå¦‚æœæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼Œå¦åˆ™False

        Raises:
            CodeExecutionTimeoutError: å½“ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶æŠ›å‡º
        """

        def timeout_handler(signum: int, frame: Any) -> None:
            raise CodeExecutionTimeoutError(f"ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆ{self.timeout_seconds}ç§’ï¼‰")

        try:
            namespace: dict[str, Any] = {}

            # è®¾ç½®è¶…æ—¶å¤„ç†
            old_handler = signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(self.timeout_seconds)

            try:
                # æ‰§è¡Œä»£ç 
                exec(code, namespace)

                # æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
                for i, test_case in enumerate(test_cases):
                    try:
                        exec(test_case, namespace)
                        logger.debug(f"æµ‹è¯•ç”¨ä¾‹ {i+1} é€šè¿‡")
                    except Exception as e:
                        logger.debug(f"æµ‹è¯•ç”¨ä¾‹ {i+1} å¤±è´¥: {e}")
                        logger.debug(f"æµ‹è¯•ç”¨ä¾‹å†…å®¹: {test_case}")
                        return False

                return True

            finally:
                # æ¢å¤åŸæ¥çš„ä¿¡å·å¤„ç†å™¨å¹¶å–æ¶ˆè¶…æ—¶
                signal.alarm(0)
                signal.signal(signal.SIGALRM, old_handler)

        except CodeExecutionTimeoutError:
            logger.debug(f"ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆ{self.timeout_seconds}ç§’ï¼‰ï¼Œå¯èƒ½å­˜åœ¨æ­»å¾ªç¯")
            return False
        except Exception as e:
            logger.debug(f"æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
            return False


class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨ç±»ã€‚"""

    @staticmethod
    @lru_cache(maxsize=1)
    def load_mbpp_dataset(max_samples: int | None = None) -> datasets.Dataset:
        """åŠ è½½MBPPæ•°æ®é›†ï¼ˆå¸¦ç¼“å­˜ï¼‰ã€‚

        Args:
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨

        Returns:
            MBPPæ•°æ®é›†

        Raises:
            DatasetLoadError: å½“æ•°æ®é›†åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            dataset = load_dataset("mbpp", split="test")
            logger.info(f"âœ… MBPPæ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")

            if max_samples is not None:
                dataset = dataset.select(range(min(max_samples, len(dataset))))
                logger.info(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡ä¸º: {max_samples}")

            return dataset

        except Exception as e:
            error_msg = f"MBPPæ•°æ®é›†åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            raise DatasetLoadError(error_msg) from e


class MBPPEvaluator:
    """MBPPè¯„ä¼°å™¨ä¸»ç±»ã€‚"""

    def __init__(self, config: EvaluationConfig) -> None:
        """åˆå§‹åŒ–è¯„ä¼°å™¨ã€‚

        Args:
            config: è¯„ä¼°é…ç½®
        """
        self.config = config
        self.model_interface = ModelInterface(config.model_path)
        self.code_executor = CodeExecutor(config.timeout_seconds)
        self.dataset_loader = DatasetLoader()

    def evaluate(self) -> EvaluationSummary:
        """æ‰§è¡Œå®Œæ•´çš„MBPPè¯„ä¼°ã€‚

        Returns:
            è¯„ä¼°æ€»ç»“ç»“æœ

        Raises:
            MBPPEvaluationError: å½“è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶æŠ›å‡º
        """
        try:
            # åŠ è½½æ•°æ®é›†
            dataset = self.dataset_loader.load_mbpp_dataset(self.config.max_samples)

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(self.config.output_dir)
            output_path.mkdir(exist_ok=True)

            # å¼€å§‹è¯„ä¼°
            logger.info(f"å¼€å§‹MBPP Instructæ¨¡å‹0-shotè¯„ä¼°ï¼ˆpass@{self.config.k}ï¼‰...")
            results: list[EvaluationResult] = []
            total_correct = 0
            start_time = time.time()

            for i, sample in enumerate(tqdm(dataset, desc="MBPPè¯„ä¼°")):
                result = self._evaluate_single_sample(sample)
                results.append(result)

                if result.test_passed:
                    total_correct += 1

                # å®šæœŸæ‰“å°è¿›åº¦
                if (i + 1) % 10 == 0:
                    current_pass_rate = total_correct / (i + 1)
                    logger.info(
                        f"è¿›åº¦: {i+1}/{len(dataset)}, å½“å‰pass@{self.config.k}: {current_pass_rate:.3f}"
                    )

            # è®¡ç®—å’Œä¿å­˜ç»“æœ
            return self._save_evaluation_results(
                results, start_time, total_correct, output_path
            )

        except Exception as e:
            if isinstance(e, MBPPEvaluationError):
                raise
            error_msg = f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}"
            logger.error(error_msg)
            raise MBPPEvaluationError(error_msg) from e

    def _evaluate_single_sample(self, sample: dict[str, Any]) -> EvaluationResult:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬ï¼ˆé’ˆå¯¹Instructæ¨¡å‹ï¼‰ã€‚

        Args:
            sample: æ•°æ®é›†ä¸­çš„å•ä¸ªæ ·æœ¬

        Returns:
            å•ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ
        """
        # æ„é€ æç¤º
        messages = self._build_prompt(sample)

        # è¿›è¡Œkæ¬¡ä»£ç ç”Ÿæˆå’Œæµ‹è¯•
        generated_codes = []
        test_results = []
        code_lengths = []
        generation_times = []

        for i in range(self.config.k):
            # ç”Ÿæˆä»£ç 
            generation_start = time.time()
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„ generate_code æ–¹æ³•
                generated_content = self.model_interface.generate_code(
                    messages, self.config.generation_config, self.config.max_new_tokens
                )

                # ä»£ç æ¸…ç†
                generated_code = self.model_interface._clean_generated_code(
                    generated_content
                )
            except CodeGenerationError as e:
                logger.warning(f"ä»»åŠ¡{sample['task_id']}ç¬¬{i+1}æ¬¡ä»£ç ç”Ÿæˆå¤±è´¥: {e}")
                generated_code = "# ä»£ç ç”Ÿæˆå¤±è´¥\npass"

            generation_time = time.time() - generation_start

            # æµ‹è¯•ä»£ç 
            test_passed = self.code_executor.test_code_execution(
                generated_code, sample["test_list"]
            )

            # å­˜å‚¨ç»“æœ
            generated_codes.append(generated_code)
            test_results.append(test_passed)
            code_lengths.append(len(generated_code))
            generation_times.append(generation_time)

        # è®¡ç®—pass@k: å¦‚æœkæ¬¡ç”Ÿæˆä¸­è‡³å°‘æœ‰ä¸€æ¬¡é€šè¿‡æµ‹è¯•ï¼Œåˆ™è¯¥æ ·æœ¬é€šè¿‡
        sample_passed = any(test_results)

        return EvaluationResult(
            task_id=sample["task_id"],
            prompt=messages,
            generated_codes=generated_codes,
            test_cases=sample["test_list"],
            test_results=test_results,
            test_passed=sample_passed,
            code_lengths=code_lengths,
            generation_times=generation_times,
        )

    @staticmethod
    def _build_prompt(sample: dict[str, Any]) -> list[dict[str, str]]:
        """æ„å»ºè¯„ä¼°æç¤ºã€‚

        Args:
            sample: æ•°æ®é›†æ ·æœ¬

        Returns:
            æ„å»ºå¥½çš„æç¤ºå­—ç¬¦ä¸²
        """
        code = sample["code"]

        # æå–å‡½æ•°ç­¾å
        for line in code.split("\n"):
            if line.strip().startswith("def"):
                signature = line
                break
        else:
            signature = ""

        # æ„å»ºå¯¹è¯æ ¼å¼çš„æç¤º
        system_prompt = "You are a helpful Python coding assistant. When given a task, you must output only the requested function code enclosed in a Python code block, without any tests or commentary."
        user_prompt = f"{sample['text']}\nThe function signature should be: {signature}"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
        return messages

    def _save_evaluation_results(
        self,
        results: list[EvaluationResult],
        start_time: float,
        total_correct: int,
        output_path: Path,
    ) -> EvaluationSummary:
        """ä¿å­˜è¯„ä¼°ç»“æœå¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯ã€‚

        Args:
            results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            start_time: è¯„ä¼°å¼€å§‹æ—¶é—´
            total_correct: é€šè¿‡çš„æ ·æœ¬æ•°é‡
            output_path: è¾“å‡ºè·¯å¾„

        Returns:
            è¯„ä¼°æ€»ç»“
        """
        total_time = time.time() - start_time
        pass_rate = total_correct / len(results) if results else 0.0

        evaluation_summary = EvaluationSummary(
            model=self.config.model_path,
            model_type="base",
            evaluation_method="0-shot",
            dataset="mbpp_original_base",
            k=self.config.k,
            total_samples=len(results),
            passed_samples=total_correct,
            pass_at_k=pass_rate,
            evaluation_time=total_time,
            average_time_per_sample=total_time / len(results) if results else 0,
        )

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ç”¨äºä¿å­˜
        summary_dict = evaluation_summary._asdict()
        results_dict = [result._asdict() for result in results]

        # ä¿å­˜æ–‡ä»¶
        results_file = output_path / "mbpp_instruct_evaluation_results.json"
        detailed_file = output_path / "mbpp_instruct_detailed_results.json"

        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary_dict, f, ensure_ascii=False, indent=2)

        with open(detailed_file, "w", encoding="utf-8") as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

        # æ‰“å°ç»“æœæ‘˜è¦
        self._print_evaluation_summary(evaluation_summary, results_file, detailed_file)

        return evaluation_summary

    @staticmethod
    def _print_evaluation_summary(
        summary: EvaluationSummary,
        results_file: Path,
        detailed_file: Path,
    ) -> None:
        """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦ã€‚

        Args:
            summary: è¯„ä¼°æ€»ç»“
            results_file: ç»“æœæ–‡ä»¶è·¯å¾„
            detailed_file: è¯¦ç»†ç»“æœæ–‡ä»¶è·¯å¾„
        """
        logger.info("=" * 50)
        logger.info(f"MBPP Instructæ¨¡å‹0-shotè¯„ä¼°å®Œæˆï¼ˆpass@{summary.k}ï¼‰ï¼")
        logger.info("=" * 50)
        logger.info(f"æ¨¡å‹: {summary.model}")
        logger.info(f"æ¨¡å‹ç±»å‹: Instruct (åè®­ç»ƒæ¨¡å‹)")
        logger.info(f"è¯„ä¼°æ–¹æ³•: 0-shot pass@{summary.k}")
        logger.info(f"æ€»æ ·æœ¬æ•°: {summary.total_samples}")
        logger.info(f"é€šè¿‡æ ·æœ¬æ•°: {summary.passed_samples}")
        logger.info(f"Pass@{summary.k}: {summary.pass_at_k:.3f}")
        logger.info(f"è¯„ä¼°ç”¨æ—¶: {summary.evaluation_time:.2f}ç§’")
        logger.info(f"å¹³å‡æ¯æ ·æœ¬: {summary.average_time_per_sample:.2f}ç§’")
        logger.info(f"ç»“æœæ–‡ä»¶: {results_file}")
        logger.info(f"è¯¦ç»†ç»“æœ: {detailed_file}")
        logger.info("=" * 50)


# ================================ å‘½ä»¤è¡Œæ¥å£ ================================


def create_arg_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚

    Returns:
        é…ç½®å¥½çš„å‚æ•°è§£æå™¨
    """
    parser = argparse.ArgumentParser(
        description="MBPP Instructæ¨¡å‹0-shot pass@kè¯„ä¼°è„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument(
        "--model",
        default="Qwen/Qwen3-0.6B",
        help="Instructæ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å",
    )

    parser.add_argument(
        "--output-dir",
        default="mbpp_results_instruct_non_thinking",
        help="è¾“å‡ºç›®å½•",
    )

    parser.add_argument(
        "--max-samples",
        type=int,
        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼Œé»˜è®¤è¯„ä¼°å…¨éƒ¨ï¼‰",
    )

    parser.add_argument(
        "--max-length",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆæ–°tokenæ•°é‡",
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=5,
        help="ä»£ç æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰",
    )

    parser.add_argument(
        "--k",
        type=int,
        default=1,
        help="pass@kè¯„ä¼°ä¸­çš„kå€¼ï¼ˆé»˜è®¤ä¸º1ï¼‰",
    )

    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ«",
    )

    return parser


def setup_logging(log_level: str) -> None:
    """è®¾ç½®æ—¥å¿—é…ç½®ã€‚

    Args:
        log_level: æ—¥å¿—çº§åˆ«
    """
    logging.basicConfig(
        level=getattr(logging, log_level),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def main() -> None:
    """ä¸»å‡½æ•°ã€‚"""
    parser = create_arg_parser()
    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)

    # åˆ›å»ºè¯„ä¼°é…ç½®
    config = EvaluationConfig(
        model_path=args.model,
        k=args.k,
        output_dir=args.output_dir,
        max_samples=args.max_samples,
        max_new_tokens=args.max_length,
        timeout_seconds=args.timeout,
    )

    try:
        # åˆ›å»ºè¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°
        evaluator = MBPPEvaluator(config)
        results = evaluator.evaluate()

        # æ‰“å°æˆåŠŸä¿¡æ¯
        print(f"\nğŸ‰ MBPP Instructæ¨¡å‹0-shotè¯„ä¼°æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“Š Pass@{results.k}: {results.pass_at_k:.3f}")
        print(f"ğŸ¤– æ¨¡å‹ç±»å‹: Instruct (åè®­ç»ƒæ¨¡å‹)")
        print(f"ğŸ¯ è¯„ä¼°æ–¹æ³•: 0-shot pass@{results.k}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}")

    except MBPPEvaluationError as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        print(f"\nâŒ MBPP Instructæ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
        exit(1)
    except Exception as e:
        logger.error(f"æœªé¢„æœŸçš„é”™è¯¯: {e}")
        print(f"\nâŒ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        exit(1)


# ================================ æ—¥å¿—é…ç½® ================================

logger = logging.getLogger(__name__)


# ================================ ä¸»ç¨‹åºå…¥å£ ================================

if __name__ == "__main__":
    main()
